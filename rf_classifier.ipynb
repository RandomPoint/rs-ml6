{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "235px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "rf_classifier.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPc9Qlj45vsJ"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxfwQRXt5vsR"
      },
      "source": [
        "Fortunately, with libraries such as Scikit-Learn, it’s now easy to build and use almost any machine learning algorithm. But it’s helpful to have an idea of how a machine learning model works under the hood. This lets us diagnose the model when it’s underperforming or explain how it makes decisions, which is crucial if we want to convince others to trust our models.\n",
        "In this assignment, we’ll look at how to build and use the Decision Tree and the Random Forest in Python. We’ll start by understanding how a single decision tree makes classifications on a simple problem. Then, we’ll work our way to using a random forest on a real-world data science problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX4o6Grg5vsS"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zml6RB1B5vsT"
      },
      "source": [
        "The dataset we will use in this assignment is the Sonar dataset.\n",
        "\n",
        "This is a dataset that describes sonar chirp returns bouncing off different surfaces. The 60 predictors are the strength of the returns at different angles. It is a binary classification problem that requires a model to differentiate rocks from metal cylinders. There are 208 observations.\n",
        "\n",
        "It is a well-understood dataset. All of the variables are continuous and generally in the range of 0 to 1. The output variable is a string “M” for mine and “R” for rock, which will need to be converted to integers 1 and 0.\n",
        "\n",
        "By predicting the class with the most observations in the dataset (M or mines) the Zero Rule Algorithm can achieve an accuracy of 53%.\n",
        "\n",
        "You can learn more about this dataset at the UCI Machine Learning repository.\n",
        "https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)\n",
        "\n",
        "Download the dataset for free and place it in the \"data\" folder in your working directory with the filename sonar.all-data.csv."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpjyrZ155vsU"
      },
      "source": [
        "# Import section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-01T08:48:35.022113Z",
          "start_time": "2020-12-01T08:48:34.335814Z"
        },
        "id": "4Gjp4-l35vsU"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "np.random.seed(2020)\n",
        "random.seed(2020)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8LsC1FZ5vsV"
      },
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOCV5FY85vsV"
      },
      "source": [
        "Read data and convert targets to integers 1 and 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-01T08:48:35.618412Z",
          "start_time": "2020-12-01T08:48:35.579547Z"
        },
        "id": "Yrgo2MZv5vsW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "e1c66cd7-b440-43fb-d5f1-3fd142de53fd"
      },
      "source": [
        "PATH = 'data/'\n",
        "df = pd.read_csv(PATH+'sonar-all-data.csv', header=None)\n",
        "df.columns = [f'feat_{col}' if col!=60 else 'target' for col in df.columns]\n",
        "df['target'] = df['target'].map({'M': 1, 'R': 0})\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feat_0</th>\n",
              "      <th>feat_1</th>\n",
              "      <th>feat_2</th>\n",
              "      <th>feat_3</th>\n",
              "      <th>feat_4</th>\n",
              "      <th>feat_5</th>\n",
              "      <th>feat_6</th>\n",
              "      <th>feat_7</th>\n",
              "      <th>feat_8</th>\n",
              "      <th>feat_9</th>\n",
              "      <th>feat_10</th>\n",
              "      <th>feat_11</th>\n",
              "      <th>feat_12</th>\n",
              "      <th>feat_13</th>\n",
              "      <th>feat_14</th>\n",
              "      <th>feat_15</th>\n",
              "      <th>feat_16</th>\n",
              "      <th>feat_17</th>\n",
              "      <th>feat_18</th>\n",
              "      <th>feat_19</th>\n",
              "      <th>feat_20</th>\n",
              "      <th>feat_21</th>\n",
              "      <th>feat_22</th>\n",
              "      <th>feat_23</th>\n",
              "      <th>feat_24</th>\n",
              "      <th>feat_25</th>\n",
              "      <th>feat_26</th>\n",
              "      <th>feat_27</th>\n",
              "      <th>feat_28</th>\n",
              "      <th>feat_29</th>\n",
              "      <th>feat_30</th>\n",
              "      <th>feat_31</th>\n",
              "      <th>feat_32</th>\n",
              "      <th>feat_33</th>\n",
              "      <th>feat_34</th>\n",
              "      <th>feat_35</th>\n",
              "      <th>feat_36</th>\n",
              "      <th>feat_37</th>\n",
              "      <th>feat_38</th>\n",
              "      <th>feat_39</th>\n",
              "      <th>feat_40</th>\n",
              "      <th>feat_41</th>\n",
              "      <th>feat_42</th>\n",
              "      <th>feat_43</th>\n",
              "      <th>feat_44</th>\n",
              "      <th>feat_45</th>\n",
              "      <th>feat_46</th>\n",
              "      <th>feat_47</th>\n",
              "      <th>feat_48</th>\n",
              "      <th>feat_49</th>\n",
              "      <th>feat_50</th>\n",
              "      <th>feat_51</th>\n",
              "      <th>feat_52</th>\n",
              "      <th>feat_53</th>\n",
              "      <th>feat_54</th>\n",
              "      <th>feat_55</th>\n",
              "      <th>feat_56</th>\n",
              "      <th>feat_57</th>\n",
              "      <th>feat_58</th>\n",
              "      <th>feat_59</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.0371</td>\n",
              "      <td>0.0428</td>\n",
              "      <td>0.0207</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0986</td>\n",
              "      <td>0.1539</td>\n",
              "      <td>0.1601</td>\n",
              "      <td>0.3109</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>0.1609</td>\n",
              "      <td>0.1582</td>\n",
              "      <td>0.2238</td>\n",
              "      <td>0.0645</td>\n",
              "      <td>0.0660</td>\n",
              "      <td>0.2273</td>\n",
              "      <td>0.3100</td>\n",
              "      <td>0.2999</td>\n",
              "      <td>0.5078</td>\n",
              "      <td>0.4797</td>\n",
              "      <td>0.5783</td>\n",
              "      <td>0.5071</td>\n",
              "      <td>0.4328</td>\n",
              "      <td>0.5550</td>\n",
              "      <td>0.6711</td>\n",
              "      <td>0.6415</td>\n",
              "      <td>0.7104</td>\n",
              "      <td>0.8080</td>\n",
              "      <td>0.6791</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>0.1307</td>\n",
              "      <td>0.2604</td>\n",
              "      <td>0.5121</td>\n",
              "      <td>0.7547</td>\n",
              "      <td>0.8537</td>\n",
              "      <td>0.8507</td>\n",
              "      <td>0.6692</td>\n",
              "      <td>0.6097</td>\n",
              "      <td>0.4943</td>\n",
              "      <td>0.2744</td>\n",
              "      <td>0.0510</td>\n",
              "      <td>0.2834</td>\n",
              "      <td>0.2825</td>\n",
              "      <td>0.4256</td>\n",
              "      <td>0.2641</td>\n",
              "      <td>0.1386</td>\n",
              "      <td>0.1051</td>\n",
              "      <td>0.1343</td>\n",
              "      <td>0.0383</td>\n",
              "      <td>0.0324</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0159</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0167</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.0523</td>\n",
              "      <td>0.0843</td>\n",
              "      <td>0.0689</td>\n",
              "      <td>0.1183</td>\n",
              "      <td>0.2583</td>\n",
              "      <td>0.2156</td>\n",
              "      <td>0.3481</td>\n",
              "      <td>0.3337</td>\n",
              "      <td>0.2872</td>\n",
              "      <td>0.4918</td>\n",
              "      <td>0.6552</td>\n",
              "      <td>0.6919</td>\n",
              "      <td>0.7797</td>\n",
              "      <td>0.7464</td>\n",
              "      <td>0.9444</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8874</td>\n",
              "      <td>0.8024</td>\n",
              "      <td>0.7818</td>\n",
              "      <td>0.5212</td>\n",
              "      <td>0.4052</td>\n",
              "      <td>0.3957</td>\n",
              "      <td>0.3914</td>\n",
              "      <td>0.3250</td>\n",
              "      <td>0.3200</td>\n",
              "      <td>0.3271</td>\n",
              "      <td>0.2767</td>\n",
              "      <td>0.4423</td>\n",
              "      <td>0.2028</td>\n",
              "      <td>0.3788</td>\n",
              "      <td>0.2947</td>\n",
              "      <td>0.1984</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.1306</td>\n",
              "      <td>0.4182</td>\n",
              "      <td>0.3835</td>\n",
              "      <td>0.1057</td>\n",
              "      <td>0.1840</td>\n",
              "      <td>0.1970</td>\n",
              "      <td>0.1674</td>\n",
              "      <td>0.0583</td>\n",
              "      <td>0.1401</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.0621</td>\n",
              "      <td>0.0203</td>\n",
              "      <td>0.0530</td>\n",
              "      <td>0.0742</td>\n",
              "      <td>0.0409</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0125</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0.0191</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>0.1099</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0974</td>\n",
              "      <td>0.2280</td>\n",
              "      <td>0.2431</td>\n",
              "      <td>0.3771</td>\n",
              "      <td>0.5598</td>\n",
              "      <td>0.6194</td>\n",
              "      <td>0.6333</td>\n",
              "      <td>0.7060</td>\n",
              "      <td>0.5544</td>\n",
              "      <td>0.5320</td>\n",
              "      <td>0.6479</td>\n",
              "      <td>0.6931</td>\n",
              "      <td>0.6759</td>\n",
              "      <td>0.7551</td>\n",
              "      <td>0.8929</td>\n",
              "      <td>0.8619</td>\n",
              "      <td>0.7974</td>\n",
              "      <td>0.6737</td>\n",
              "      <td>0.4293</td>\n",
              "      <td>0.3648</td>\n",
              "      <td>0.5331</td>\n",
              "      <td>0.2413</td>\n",
              "      <td>0.5070</td>\n",
              "      <td>0.8533</td>\n",
              "      <td>0.6036</td>\n",
              "      <td>0.8514</td>\n",
              "      <td>0.8512</td>\n",
              "      <td>0.5045</td>\n",
              "      <td>0.1862</td>\n",
              "      <td>0.2709</td>\n",
              "      <td>0.4232</td>\n",
              "      <td>0.3043</td>\n",
              "      <td>0.6116</td>\n",
              "      <td>0.6756</td>\n",
              "      <td>0.5375</td>\n",
              "      <td>0.4719</td>\n",
              "      <td>0.4647</td>\n",
              "      <td>0.2587</td>\n",
              "      <td>0.2129</td>\n",
              "      <td>0.2222</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>0.0176</td>\n",
              "      <td>0.1348</td>\n",
              "      <td>0.0744</td>\n",
              "      <td>0.0130</td>\n",
              "      <td>0.0106</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0164</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0078</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0171</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1276</td>\n",
              "      <td>0.0598</td>\n",
              "      <td>0.1264</td>\n",
              "      <td>0.0881</td>\n",
              "      <td>0.1992</td>\n",
              "      <td>0.0184</td>\n",
              "      <td>0.2261</td>\n",
              "      <td>0.1729</td>\n",
              "      <td>0.2131</td>\n",
              "      <td>0.0693</td>\n",
              "      <td>0.2281</td>\n",
              "      <td>0.4060</td>\n",
              "      <td>0.3973</td>\n",
              "      <td>0.2741</td>\n",
              "      <td>0.3690</td>\n",
              "      <td>0.5556</td>\n",
              "      <td>0.4846</td>\n",
              "      <td>0.3140</td>\n",
              "      <td>0.5334</td>\n",
              "      <td>0.5256</td>\n",
              "      <td>0.2520</td>\n",
              "      <td>0.2090</td>\n",
              "      <td>0.3559</td>\n",
              "      <td>0.6260</td>\n",
              "      <td>0.7340</td>\n",
              "      <td>0.6120</td>\n",
              "      <td>0.3497</td>\n",
              "      <td>0.3953</td>\n",
              "      <td>0.3012</td>\n",
              "      <td>0.5408</td>\n",
              "      <td>0.8814</td>\n",
              "      <td>0.9857</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>0.6121</td>\n",
              "      <td>0.5006</td>\n",
              "      <td>0.3210</td>\n",
              "      <td>0.3202</td>\n",
              "      <td>0.4295</td>\n",
              "      <td>0.3654</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>0.1576</td>\n",
              "      <td>0.0681</td>\n",
              "      <td>0.0294</td>\n",
              "      <td>0.0241</td>\n",
              "      <td>0.0121</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0117</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0762</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.0394</td>\n",
              "      <td>0.0590</td>\n",
              "      <td>0.0649</td>\n",
              "      <td>0.1209</td>\n",
              "      <td>0.2467</td>\n",
              "      <td>0.3564</td>\n",
              "      <td>0.4459</td>\n",
              "      <td>0.4152</td>\n",
              "      <td>0.3952</td>\n",
              "      <td>0.4256</td>\n",
              "      <td>0.4135</td>\n",
              "      <td>0.4528</td>\n",
              "      <td>0.5326</td>\n",
              "      <td>0.7306</td>\n",
              "      <td>0.6193</td>\n",
              "      <td>0.2032</td>\n",
              "      <td>0.4636</td>\n",
              "      <td>0.4148</td>\n",
              "      <td>0.4292</td>\n",
              "      <td>0.5730</td>\n",
              "      <td>0.5399</td>\n",
              "      <td>0.3161</td>\n",
              "      <td>0.2285</td>\n",
              "      <td>0.6995</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.7262</td>\n",
              "      <td>0.4724</td>\n",
              "      <td>0.5103</td>\n",
              "      <td>0.5459</td>\n",
              "      <td>0.2881</td>\n",
              "      <td>0.0981</td>\n",
              "      <td>0.1951</td>\n",
              "      <td>0.4181</td>\n",
              "      <td>0.4604</td>\n",
              "      <td>0.3217</td>\n",
              "      <td>0.2828</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.1979</td>\n",
              "      <td>0.2444</td>\n",
              "      <td>0.1847</td>\n",
              "      <td>0.0841</td>\n",
              "      <td>0.0692</td>\n",
              "      <td>0.0528</td>\n",
              "      <td>0.0357</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.0046</td>\n",
              "      <td>0.0156</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0107</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   feat_0  feat_1  feat_2  feat_3  ...  feat_57  feat_58  feat_59  target\n",
              "0  0.0200  0.0371  0.0428  0.0207  ...   0.0084   0.0090   0.0032       0\n",
              "1  0.0453  0.0523  0.0843  0.0689  ...   0.0049   0.0052   0.0044       0\n",
              "2  0.0262  0.0582  0.1099  0.1083  ...   0.0164   0.0095   0.0078       0\n",
              "3  0.0100  0.0171  0.0623  0.0205  ...   0.0044   0.0040   0.0117       0\n",
              "4  0.0762  0.0666  0.0481  0.0394  ...   0.0048   0.0107   0.0094       0\n",
              "\n",
              "[5 rows x 61 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDB8Gg-t5vsX"
      },
      "source": [
        "# Split data (train and test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-01T08:48:36.554013Z",
          "start_time": "2020-12-01T08:48:36.548028Z"
        },
        "id": "z0Uvv_1U5vsY"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns='target'), df['target'], test_size=0.2, random_state=2020)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytahaIsE5vsY"
      },
      "source": [
        "# Cost functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVqZHfG45vsZ"
      },
      "source": [
        "In this section you should implement two cost functions. Any of these can be used in the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yozKNSUH5vsZ"
      },
      "source": [
        "## Gini index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-01T08:48:38.113015Z",
          "start_time": "2020-12-01T08:48:38.108028Z"
        },
        "id": "3tUEMyZA5vsZ"
      },
      "source": [
        "def gini_index(x):\n",
        "    \"\"\" Calculate Gini Index for a node\n",
        "    Args:\n",
        "        x: Numpy-array of targets in a node\n",
        "    Returns:\n",
        "        float: Gini index\n",
        "    \"\"\"    \n",
        "    if len(x) == 0:\n",
        "        return 0.0\n",
        "    p = np.bincount(x) / len(x)\n",
        "    return 1 - np.sum(p*p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-01T08:48:38.326622Z",
          "start_time": "2020-12-01T08:48:38.320603Z"
        },
        "id": "hEKqpFYH5vsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd9df443-b8bd-40e5-b579-5a33c975444a"
      },
      "source": [
        "target = df['target'].values\n",
        "gini_index(target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4977348372781065"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-01T08:48:38.517903Z",
          "start_time": "2020-12-01T08:48:38.512916Z"
        },
        "id": "Cm9NjU0m5vsa"
      },
      "source": [
        "def gini_gain(parent_node, splits):\n",
        "    \"\"\" Calculate Gini Gain for a particular split\n",
        "    Args:\n",
        "        parent_node: Numpy-array of targets in a parent node\n",
        "        splits: List of two numpy-arrays. Each numpy-array is targets in a child node\n",
        "    Returns:\n",
        "        float: Gini gain\n",
        "    \"\"\"       \n",
        "    splits_gini = np.sum([gini_index(split)*(len(split)/len(parent_node)) for split in splits])\n",
        "    return gini_index(parent_node) - splits_gini"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-01T08:48:38.731542Z",
          "start_time": "2020-12-01T08:48:38.719612Z"
        },
        "id": "C40WU0I45vsb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b920fb94-f2d8-45b5-89fa-0270137cb240"
      },
      "source": [
        "splits = [np.random.choice(df['target'].values, 100), np.random.choice(df['target'].values, 108)]\n",
        "gini_gain(target, splits)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.0003705758273065962"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OGtkjOv5vsb"
      },
      "source": [
        "## Entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-01T08:48:39.080145Z",
          "start_time": "2020-12-01T08:48:39.075122Z"
        },
        "id": "ATxiRZen5vsb"
      },
      "source": [
        "def entropy(x):\n",
        "    \"\"\" Calculate Entropy for a node\n",
        "    Args:\n",
        "        x: Numpy-array of targets in a node\n",
        "    Returns:\n",
        "        float: Entropy\n",
        "    \"\"\"\n",
        "    if len(x) == 0:\n",
        "        return 0.0\n",
        "    p = np.clip(np.bincount(x) / len(x), 1e-15, 1.)\n",
        "    return -np.sum(p * np.log(p))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-01T08:48:39.415230Z",
          "start_time": "2020-12-01T08:48:39.410243Z"
        },
        "id": "gJe4djzz5vsc"
      },
      "source": [
        "def information_gain(parent_node, splits):\n",
        "    \"\"\" Calculate Information Gain for a particular split\n",
        "    Args:\n",
        "        parent_node: Numpy-array of targets in a parent node\n",
        "        splits: List of two numpy-arrays. Each numpy-array is targets in a child node\n",
        "    Returns:\n",
        "        float: Information Gain\n",
        "    \"\"\"     \n",
        "    splits_entropy = np.sum([entropy(split)*(len(split)/len(parent_node)) for split in splits])\n",
        "    return entropy(parent_node) - splits_entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu1U031a5vsc"
      },
      "source": [
        "# Split function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cd333My5vsd"
      },
      "source": [
        "Implement split functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-01T08:48:40.068764Z",
          "start_time": "2020-12-01T08:48:40.063788Z"
        },
        "id": "hphCQy3I5vsd"
      },
      "source": [
        "def split(X, y, value):\n",
        "    \"\"\" Split y-values in order to calculate gain later\n",
        "    Args:\n",
        "        X: 1-dimensional numpy-array of data predictor with shape (N,)\n",
        "        y: 1-dimensional numpy-array of targets with shape (N,)\n",
        "        value (float): the value by which the X should be splitted\n",
        "    Returns:\n",
        "        Two 1-dimensional numpy-arrays with targets related to splits\n",
        "    \"\"\"      \n",
        "    left_mask = X < value\n",
        "    right_mask = X >= value\n",
        "    return y[left_mask], y[right_mask]\n",
        "\n",
        "\n",
        "def split_dataset(X, y, column, value):\n",
        "    \"\"\" Split dataset by a particular column and value\n",
        "    Args:\n",
        "        X: 2-dimensional numpy-array (N, num_feats). N-number of samples\n",
        "        y: 1-dimensional numpy-array of targets with shape (N,)  \n",
        "        column (int): the column by which the X should be splitted\n",
        "        value (float): the value by which the column should be splitted\n",
        "    Returns:\n",
        "        Two 2-dimensional numpy-arrays with data and two 1-dimensional numpy-arrays with targets related to splits\n",
        "        left_X, right_X, left_y, right_y\n",
        "    \"\"\"       \n",
        "    left_mask = X[:, column] < value\n",
        "    right_mask = X[:, column] >= value\n",
        "    left_y, right_y = y[left_mask], y[right_mask]\n",
        "    left_X, right_X = X[left_mask], X[right_mask]\n",
        "    return left_X, right_X, left_y, right_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiZxk9-65vse"
      },
      "source": [
        "# Decision tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-01T08:48:40.412627Z",
          "start_time": "2020-12-01T08:48:40.393648Z"
        },
        "id": "u-hEFo-q5vse"
      },
      "source": [
        "class Tree(object):\n",
        "    \"\"\"A decision tree classifier.\n",
        "\n",
        "    Args:\n",
        "        criterion : {\"gini_gain\", \"information_gain\"}\n",
        "    \"\"\"\n",
        "    def __init__(self, criterion=None):\n",
        "        self.impurity = None\n",
        "        self.threshold = None\n",
        "        self.column_index = None\n",
        "        self.outcome_probs = None\n",
        "        self.criterion = criterion\n",
        "        self.left_child = None\n",
        "        self.right_child = None\n",
        "\n",
        "    @property\n",
        "    def is_terminal(self):\n",
        "        \"\"\" Define is it terminal node\n",
        "        \"\"\"          \n",
        "        return not bool(self.left_child and self.right_child)\n",
        "\n",
        "    def _find_splits(self, X):\n",
        "        \"\"\"Find all possible split values.\"\"\"\n",
        "        split_values = set()\n",
        "\n",
        "        # Get unique values in a sorted order\n",
        "        x_unique = list(np.unique(X))\n",
        "        for i in range(1, len(x_unique)):\n",
        "            # Find a point between two values\n",
        "            average = (x_unique[i - 1] + x_unique[i]) / 2.0\n",
        "            split_values.add(average)\n",
        "\n",
        "        return list(split_values)\n",
        "\n",
        "    def _find_best_split(self, X, y, n_features):\n",
        "        \"\"\"Find best feature and value for a split. Greedy algorithm.\"\"\"\n",
        "\n",
        "        # Sample random subset of features\n",
        "        subset = random.sample(list(range(0, X.shape[1])), n_features)\n",
        "        max_gain, max_col, max_val = None, None, None\n",
        "\n",
        "        for column in subset:\n",
        "            split_values = self._find_splits(X[:, column])\n",
        "            for value in split_values:\n",
        "                splits = split(X[:, column], y, value)\n",
        "                gain = self.criterion(y, splits)\n",
        "\n",
        "                if (max_gain is None) or (gain > max_gain):\n",
        "                    max_col, max_val, max_gain = column, value, gain\n",
        "        return max_col, max_val, max_gain\n",
        "\n",
        "    def fit(self, X, y, n_features=None, max_depth=None):\n",
        "        \"\"\"Fit model.\n",
        "\n",
        "        Args:\n",
        "            X (numpy-array): The training input samples. 2-dimensional numpy array.\n",
        "            y (numpy-array): The target values. 1-dimensional numpy array.\n",
        "            n_features (int): The number of features when fit is performed (default: all features)\n",
        "            max_depth (int): The maximum depth of the tree. If None, then nodes are expanded until\n",
        "                             all leaves are pure.\n",
        "        \"\"\"        \n",
        "        try:\n",
        "            # Exit from recursion using assert syntax\n",
        "            if max_depth is not None:\n",
        "                assert max_depth > 0\n",
        "                max_depth -= 1\n",
        "\n",
        "            if n_features is None:\n",
        "                n_features = X.shape[1]\n",
        "\n",
        "            column, value, gain = self._find_best_split(X, y, n_features)\n",
        "            assert gain is not None\n",
        "\n",
        "            self.column_index = column\n",
        "            self.threshold = value\n",
        "            self.impurity = gain\n",
        "\n",
        "            # Split dataset\n",
        "            left_X, right_X, left_target, right_target = split_dataset(X, y, column, value)\n",
        "\n",
        "            # Grow left and right child\n",
        "            self.left_child = Tree(self.criterion)\n",
        "            self.left_child.fit(\n",
        "                left_X, left_target, n_features, max_depth\n",
        "            )\n",
        "\n",
        "            self.right_child = Tree(self.criterion)\n",
        "            self.right_child.fit(\n",
        "                right_X, right_target, n_features, max_depth\n",
        "            )\n",
        "        except AssertionError:\n",
        "            self.outcome_probs = np.around(np.sum(y) / y.shape[0])\n",
        "\n",
        "\n",
        "    def predict_row(self, row):\n",
        "        \"\"\"Predict single row.\"\"\"\n",
        "        if not self.is_terminal:\n",
        "            if row[self.column_index] < self.threshold:\n",
        "                return self.left_child.predict_row(row)\n",
        "            else:\n",
        "                return self.right_child.predict_row(row)\n",
        "        return self.outcome_probs\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions.\n",
        "\n",
        "        Args:\n",
        "            X (numpy-array): The test input samples. 2-dimensional numpy array.\n",
        "        \"\"\"  \n",
        "        result = np.zeros(X.shape[0])\n",
        "        for i in range(X.shape[0]):\n",
        "            result[i] = self.predict_row(X[i, :])\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoBLIn1m5vsj"
      },
      "source": [
        "Fit two models with \"max_depth=3\" and \"max_depth=None\" hyperparameters. Explain the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-01T08:48:45.836138Z",
          "start_time": "2020-12-01T08:48:41.916791Z"
        },
        "id": "dn_WwE-e5vso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88fe4a0c-eeba-44db-fb38-064d7b3ceaab"
      },
      "source": [
        "model = Tree(criterion=gini_gain)\n",
        "model.fit(X_train.values, y_train.values)\n",
        "y_pred = model.predict(X_test.values)\n",
        "print(f\"Accuracy score is: {accuracy_score(y_test, y_pred)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score is: 0.6666666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-01T08:48:41.915820Z",
          "start_time": "2020-12-01T08:48:40.891474Z"
        },
        "id": "E1lxMyC25vso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23531145-cf4d-43bc-ee88-e58b95413492"
      },
      "source": [
        "model = Tree(criterion=gini_gain)\n",
        "model.fit(X_train.values, y_train.values, max_depth=3)\n",
        "y_pred = model.predict(X_test.values)\n",
        "print(f\"Accuracy score is: {accuracy_score(y_test, y_pred)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score is: 0.6666666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB63F-tF5vsp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4dde117-c887-48a0-8d82-0d505fc557d1"
      },
      "source": [
        "model = Tree(criterion=information_gain)\n",
        "model.fit(X_train.values, y_train.values, max_depth=3)\n",
        "y_pred = model.predict(X_test.values)\n",
        "print(f\"Accuracy score is: {accuracy_score(y_test, y_pred)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score is: 0.7380952380952381\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgr58qiy5vsp"
      },
      "source": [
        "# Random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-01T08:48:45.857086Z",
          "start_time": "2020-12-01T08:48:45.838139Z"
        },
        "id": "nCA3q5g45vsp"
      },
      "source": [
        "class RandomForestClassifier(object):\n",
        "    \"\"\"\n",
        "    A random forest classifier.\n",
        "    A random forest is a meta estimator that fits a number of decision tree\n",
        "    classifiers on various sub-samples of the dataset and uses averaging to\n",
        "    improve the predictive accuracy and control overfitting.\n",
        "    \n",
        "    Args:\n",
        "        n_estimators : int, default=10\n",
        "            The number of trees in the forest.\n",
        "\n",
        "        max_depth : int, default=None\n",
        "            The maximum depth of the tree. If None, then nodes are expanded until\n",
        "            all leaves are pure.        \n",
        "\n",
        "        n_features : int, default=None\n",
        "            The number of features to consider when looking for the best split.\n",
        "            If None, then `n_features=sqrt(n_features)`.\n",
        "\n",
        "        criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
        "            The function to measure the quality of a split. Supported criteria are\n",
        "            \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_estimators=10, max_depth=None, n_features=None, criterion=\"entropy\", bootstrap=True):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.n_features = n_features\n",
        "        self.bootstrap = bootstrap\n",
        "        \n",
        "        if criterion == \"entropy\":\n",
        "            self.criterion = information_gain\n",
        "        elif criterion == \"gini\":\n",
        "            self.criterion = gini_gain\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown criterion '{criterion}'\")\n",
        "            \n",
        "        self.trees = [Tree(criterion=self.criterion) for _ in range(n_estimators)]\n",
        "        \n",
        "    def _init_data(self, X, y):\n",
        "        \"\"\"Ensure data are in the expected format.\n",
        "        Ensures X and y are stored as numpy ndarrays by converting from an\n",
        "        array-like object if necessary. \n",
        "        Parameters\n",
        "        Args:\n",
        "            X : array-like\n",
        "                Feature dataset.\n",
        "            y : array-like, default=None\n",
        "                Target values. By default is required, but if y_required = false\n",
        "                then may be omitted.\n",
        "        \"\"\"\n",
        "        self.size = len(X)\n",
        "        \n",
        "        if not isinstance(X, np.ndarray):\n",
        "            self.X = np.array(X)\n",
        "        else:\n",
        "            self.X = X\n",
        "\n",
        "        if not isinstance(y, np.ndarray):\n",
        "            self.y = np.array(y)\n",
        "        else:\n",
        "            self.y = y\n",
        "            \n",
        "    def bootstrap_data(self, size):\n",
        "        return np.random.randint(size, size=size)\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit model.\n",
        "\n",
        "        Args:\n",
        "            X (numpy-array): The training input samples. 2-dimensional numpy array.\n",
        "            y (numpy-array): The target values. 1-dimensional numpy array.\n",
        "        \"\"\"         \n",
        "        if self.n_features is None:\n",
        "            self.n_features = int(np.sqrt(X.shape[1]))\n",
        "        elif X.shape[1] < self.n_features:\n",
        "            raise ValueError(f\"'n_features should be <= n_features'\")\n",
        "            \n",
        "        self._init_data(X, y)\n",
        "        \n",
        "        for tree in self.trees:\n",
        "            if self.bootstrap:\n",
        "                idxs = self.bootstrap_data(self.size)\n",
        "                X = self.X[idxs]\n",
        "                y = self.y[idxs]\n",
        "            else:\n",
        "                X = self.X\n",
        "                y = self.y\n",
        "                \n",
        "            tree.fit(\n",
        "                X,\n",
        "                y,\n",
        "                n_features=self.n_features,\n",
        "                max_depth=self.max_depth,\n",
        "            )\n",
        "            \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions.\n",
        "\n",
        "        Args:\n",
        "            X (numpy-array): The test data input samples. 2-dimensional numpy array.\n",
        "        \"\"\"            \n",
        "        if not isinstance(X, np.ndarray):\n",
        "            X = np.array(X)\n",
        "\n",
        "        if self.X is not None:\n",
        "            predictions = np.zeros(len(X))\n",
        "            for i in range(len(X)):\n",
        "                row_pred = 0.\n",
        "                for tree in self.trees:\n",
        "                    row_pred += tree.predict_row(X[i, :])\n",
        "\n",
        "                row_pred /= self.n_estimators\n",
        "                predictions[i] = round(row_pred)\n",
        "            return predictions  \n",
        "        else:\n",
        "            raise ValueError(\"You should fit a model before `predict`\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd0jjjj95vsq"
      },
      "source": [
        "Fit two models with \"n_estimators=10\" and \"n_estimators=100\" hyperparameters. Explain the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-01T08:48:51.641967Z",
          "start_time": "2020-12-01T08:48:45.860076Z"
        },
        "id": "lWQzshkI5vsq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8869af03-5816-4c14-8278-d61d5056c4ef"
      },
      "source": [
        "model = RandomForestClassifier(n_estimators=10, max_depth=None, n_features=None, criterion=\"entropy\")\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(f\"Accuracy score is: {accuracy_score(y_test, y_pred)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score is: 0.7380952380952381\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-01T08:49:47.690531Z",
          "start_time": "2020-12-01T08:48:51.644919Z"
        },
        "id": "RoCBr38O5vsq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92a79c2b-f44f-47ec-fa2a-e1c69ce8ba38"
      },
      "source": [
        "model = RandomForestClassifier(n_estimators=100, max_depth=None, n_features=None, criterion=\"entropy\")\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(f\"Accuracy score is: {accuracy_score(y_test, y_pred)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score is: 0.8809523809523809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-btFtpJx5vsr"
      },
      "source": [
        "Now it's your turn to explore the various parameters of sklearn [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and their influence on model quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP9zpVUW5vsr"
      },
      "source": [
        "# Homework part 1. RandomForestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSG1ysBS5vsr"
      },
      "source": [
        "_Note_: Consider **accuracy** as main metric of model performance on this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2_zXsjt5vss"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbPxXSxS5vss"
      },
      "source": [
        "**Task 1 (0.5 points)** Split the dataset into train, test and validation parts (0.6 / 0.2 / 0.2). First two will be used for model hyperparameter tuning whereas the best model quality should be evaluated on validation part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMDj1S4U5vss"
      },
      "source": [
        "# Your code here\r\n",
        "X, X_val, y, y_val = train_test_split(df.drop(columns='target'), df['target'], test_size=0.2, random_state=2020)\r\n",
        "#здесь X - датасет, включающий в себя обучающую и тестовую части\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2020)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgfAn9ZQ5vss"
      },
      "source": [
        "**Task 2 (2 points)**. Apply '_greedy_' hyperparameter tuning of RandomForestClassifier model. '_Greedy_' way means the following general approach. At first we tune one model parameter whereas others are fixed with default values. Then we move on to tune the second parameter whereas others are fixed default values and first has the best values from previous step. After it we tune the third parameter with best fixed values for previous two and default values for the rest. Repeat until we go through all the parameters, then repeat this cycle if you are seeing a clear increase in the test metric. <br>\n",
        "\n",
        "Although this approach has a lot of disadvantages (you may think which ones), sometimes that is the only way to tune model typerparams due to big training time **if you understand how the model parameters are interrelated and the tuning order takes those dependencies into account.**<br>\n",
        "\n",
        "Here is one of the possible options for RandomForestClassifier:\n",
        "- Choose a decent value for number of trees using '_elbow_' rule. You may plot the dependence of accuracy on trees_num and pick up the number after which the error decreases not **as much as before**. \n",
        "- Pick up the best split criterion ('gini' / 'entropy') and then tune _max_depth_, _min_samples_split_, _min_samples_leaf_.\n",
        "- Increase number of trees with best found parameters so far.\n",
        "- Repeat this excersice starting from picking the best split criterion while other params are fixed with best values from previous steps **if you observe a significant test metric improvement**. Otherwise just stop and measure your best model result on validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeJ1iZ3j5vst"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zOjh1He5vst"
      },
      "source": [
        "### Grid Search and Random Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vdiq4QG_5vst"
      },
      "source": [
        "If you have enough computational power for model training in a reasonable amount of time more sophisticated approach of hyperparameter tuning would be either Grid Search or Random Search.<br>\n",
        "\n",
        "In a nutshell Grid Search allows you to pass through all different combinations of given model parameters and their values and choose the best combination. Whereas Random Search would randomly choose values for given model parameters and evaluate them on test data untill it reaches the specified number of iterations.<br>\n",
        "\n",
        "More information here [Gentle introduction to Grid and Random search](https://medium.com/@senapati.dipak97/grid-search-vs-random-search-d34c92946318) and here [Detailed Explanation with code examples](https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pJFGOlg5vst"
      },
      "source": [
        "![grid_random_search.png](attachment:grid_random_search.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSRUuK47NJtE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HgvA48t5vsu"
      },
      "source": [
        "**Task 3 (1 point)**. Compare your previous results with [GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) hyperparameter tuning. You may tune best hyperparameters for forest with several trees and then increase it while measure the quality on validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJFduGzB5vsu"
      },
      "source": [
        "# Your code here\r\n",
        "classifier = RandomForestClassifier()\r\n",
        "param_space = dict()\r\n",
        "param_space['n_estimators'] = [10, 100, 300, 500, 1000]\r\n",
        "param_space['max_depth'] = [5, 8, 11, 14]\r\n",
        "param_space['min_samples_split'] = [2, 5, 10, 15, 100]\r\n",
        "param_space['min_samples_leaf'] = [1, 2, 5]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTDxhQoUODsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5edfeb8b-17d2-4d50-b61e-f4c6c9bf0bb1"
      },
      "source": [
        "clf_grid = GridSearchCV(classifier, param_space, cv = 3, verbose = 1, n_jobs = -1)\r\n",
        "clf_best_grid = clf_grid.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   23.3s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  3.6min\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:  6.5min\n",
            "[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed:  7.4min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZbdkH4jU59_",
        "outputId": "c03cf394-5788-4457-cd79-a849eb800749"
      },
      "source": [
        "print(clf_grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'max_depth': 8, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C42HNSFtT2Ok",
        "outputId": "2b5a65fb-65dd-4f97-d0e1-56f33a4e6a9b"
      },
      "source": [
        "y_pred_grid = clf_best_grid.predict(X_test)\r\n",
        "print(f\"Accuracy score for GridSearch is: {accuracy_score(y_test, y_pred_grid)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score is: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOH_cz985vsu"
      },
      "source": [
        "**Task 4 (1 point)**. And finally tune forest hyperparameters with [RandomSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html). Compare results to previous attempts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erqmPilP5vsu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ea989b8-8476-474c-e673-47f29589fb8e"
      },
      "source": [
        "# Your code here\r\n",
        "clf_random = RandomizedSearchCV(classifier, param_space, cv = 3, verbose = 1, n_jobs = -1)\r\n",
        "clf_best_random = clf_random.fit(X, y)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   12.9s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twxtPXQyqamL",
        "outputId": "38973338-8ac5-4367-99cd-981866470f4a"
      },
      "source": [
        "print(clf_random.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoH1N-jKqfzN",
        "outputId": "208e190d-9362-4812-9313-845840a83832"
      },
      "source": [
        "y_pred_random = clf_best_random.predict(X_test)\r\n",
        "print(f\"Accuracy score for RandomSearch is: {accuracy_score(y_test, y_pred_random)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score for RandomSearch is: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjEsteEO5vsu"
      },
      "source": [
        "**Task 5 (0.5 points)**. Tell us about your experience in hyperparameter tuning with the approaches above. What do you think would be the best option for this task and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJaXu1715vsv"
      },
      "source": [
        "Your cool ideas here..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6s5ZUAW5vsv"
      },
      "source": [
        "### Desicion tree explained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8F8hDML5vsv"
      },
      "source": [
        "Remember the [Titanic](https://www.kaggle.com/c/titanic) competition from last week? Wouldn't be a good idea to visualize one of possible desicion-making processes of _survived_ / _dead_ labeling?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAvJGpHm5vsv"
      },
      "source": [
        "**Task 6 (1 point)**. Load titanic dataset, split it into train/test parts, apply simple hyperparameter tuning of [DesicionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) (use one of the approaches above) in order to have **test accuracy more than 0.65**. <br>\n",
        "\n",
        "Draw the best tree decision making process. You may use [sklearn.tree.prot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html).<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9zPqmFT5vsw"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ome07sP5vsw"
      },
      "source": [
        "Is it easy to interpret its results? Are you able to explain to a random person why would he survive / die on the titanic?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI1_eh-x5vsw"
      },
      "source": [
        "# Homework part 2. RandomForestRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJsImsTG5vsw"
      },
      "source": [
        "**Task 7 (2 points)**. Write your own *DecisionTreeRegressor* class with _MSE_ split criterion and settable parameter *max_depth*. Demonstrate its consistency on the proposed artificial data (or some other) by comparing MSE of train predictions with [sklearn.tree.DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html). <br>\n",
        "\n",
        "Provide examples for different _max_depth_ parameter.\n",
        "\n",
        "Of course you may re-use code for *DecisionTreeClassifier*. You need to figure out what needs to be changed in it for Classification -> Regression transformation.<br>\n",
        "\n",
        "**! You are allowed to use only NumPy library** in this assigment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSZXfdfH5vsx"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def f(X):\n",
        "    return X[:, 0]**3 + np.log(np.exp(X[:, 1]) + np.exp(X[:, 2])) + np.sqrt(abs(X[:, 3])) * X[:, 4]\n",
        "\n",
        "n_samples = 100\n",
        "\n",
        "stdv = 1. / np.sqrt(5)\n",
        "\n",
        "X = np.random.uniform(-stdv, stdv, size = (n_samples, 5))\n",
        "y = f(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Yd_TEgz5vsx"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LflEXrCE5vsx"
      },
      "source": [
        "**Task 8 (2 points)**. Write your own _RandomForestRegressor_ class with MSE split criterion and settable parameter _max_depth_.  Demonstrate its consistency on the proposed artificial data (or some other) by comparing MSE of train predictions with [sklearn.ensemble.RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html).<br>\n",
        "\n",
        "Provide examples for different max_depth parameter.<br>\n",
        "\n",
        "**! You are allowed to use only NumPy library** in this assigment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFHvzDdU5vsx"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r7o0fE_5vsy"
      },
      "source": [
        "# Homework part 3 (bonus). Speeding up forest training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nUGov-O5vsy"
      },
      "source": [
        "**Task 9 (3 points)** Devise a way to speed up training against the default version of our custom _RandomForestClassifier_ or your own _RandomForestRegressor_. You may want use [`joblib`](https://joblib.readthedocs.io/en/latest/) for parallelizing trees training. Provide graphs of time dependences on the number of trees in your _fast_ version with different number of cores / threads used against default one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWg-URO95vsy"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}